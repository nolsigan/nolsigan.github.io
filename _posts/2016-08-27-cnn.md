---
layout: post
date: 2016-08-27
title: Convolutional Neural Network
---

### Deep learning

하나의 뉴런이 어떤 특정 feature를 학습한다고 이해하면 hidden layer의 깊이가 깊어질수록 알고리즘이 더 좋은 결과를 낼 것이라 생각할 수 있다.
대부분 인간이 이해하는 문제들은 복잡하고 여러 단계의 추상화를 거쳐 결론에 도달하므로 deep network가 더 좋은 결과를 내는 것은 상식적이다.

문제는 신경망이 깊어질수록 학습시키기가 어렵다는 건데 이전에 언급한 unstable gradient 현상이 발생할 확률이 증가하기 때문이다.
따라서 좋은 결과를 얻기 위해선 단순히 fully-connected 된 deep network로는 어렵고 여러 방법들을 사용하게 되는데 image recognition 분야에서
가장 좋은 성과를 내고 있는 것은 바로 Convolutional neural network이다.


### Convolutional Neural Networks

CNN이 문제 해결에 접근하는 방식은 상당히 직관적이고 설득력 있다. Image의 경우 그 주변 pixel과의 관계가 어떤 image인 지 판단하는데 중요하다는 사실에서 착안한다.

#### Local receptive fields

Pixel을 부분별로 나눠 판단하는 것에서부터 시작한다. 예로 5x5 크기의 정사각형 모양의 pixel 값들이 하나의 뉴런을 학습시킨다. 이 때, 이 pixel들은 공통의 weight을 사용하므로써
하나의 feature를 뉴런이 학습할 수 있도록 한다. 정사각형 단위를 옆으로 정해진 만큼 슬라이딩 시켜 그 다음 뉴런에 연결한다. 정사각형 변의 길이가 5, 슬라이딩 단위가 1라면 28x28 크기의
input layer는 24x24의 first hidden layer를 갖게 된다. 이를 하나의 feature를 얻는다고도 하며 적게는 3개, 많게는 20개가 넘게 feature를 얻기도 한다.

#### Max pooling

Local receptive fields를 통해 얻어진 layer에서 상대적으로 영향력이 쎈 뉴런들만 선택하여 learning saturation을 방지하는 방법이 max pooling이다. 2x2 단위로 적용하면
24x24의 layer는 12x12가 된다.

#### Putting it all together

위 두 과정을 n번 (보통 <=3) 반복한 layer를 fully-connected 된 hidden layer에 연결하여 알고리즘을 완성시킨다. 뒤의 fully-connected된 부분이 이전과 모양이 같음을 생각하면
결국 앞의 처리 부분이 saturation을 줄이기 위한 input 사이즈 줄이기라고도 볼 수 있겠다. 다만 이 과정에도 학습과정이 포함 되므로 인간의 판단에 의한 feature를 설정하는게 아니라
학습된 feature를 설정하므로 보편성이 유지 된다.
             

### Recent progress in image recognition

2016년 기준, MNIST 데이터에 대한 알고리즘의 정확도는 99.79까지 상승하였다. 틀린 답들의 경우, 인간도 구별하기 힘들 정도의 케이스라는 것을 생각하면 MNIST는 완전히 정복했다고 할 수 있겠다.

더 복잡한 문제로는 ImageNet의 데이터를 classification 하는 것인데, 이 분야는 아직 완벽하지 않아서 가장 근접한 5개로 구별하기와 같은 방법을 쓰더라도 80% 정도의 정확도가 나온다. 연구마다
차이점은 있지만 대부분 공통적으로 back propagation, regularization, dropout, sgd 등의 방법을 쓴다.


### 아직 갈 길이 멀다

처음 인공 신경망이란 아이디어가 나왔을 땐, 이를 통해 인간의 뇌를 이해하게 되는 것을 기대하였지만 현실은 인공 신경망조차 이해하지 못하고 있다. 연구가 발전하는 과정 또한
어떤 수학적 이론을 바탕으로한 결과물이 아니라 실제로 해 봤을 때 결과가 잘 나오는 것을 다들 택하는 식이다. 예를 들어, 많은 최근 연구들이 ReLu를 activation function으로 사용하는데,
다른 함수에 비해 속도가 빠름은 자명하지만 왜 좋은 퍼포먼스를 내는지는 이해하지 못하고 있다.

결과 중심적인 연구 흐름은 긍정적일 순 없지만, 좋은 결과를 낸 방법을 바탕으로 새로운 중요한 근본적인 문제를 발견하고 이를 해결하는 방식으로 흐르고 있기 때문에 부정적일 것도 없다고 생각된다.
중요한 것은 neural net이 아직 그 한계가 발견되지 않았다는 것이며, 이를 통해 해결할 수 있는 문제가 너무나 많이 남았다는 것이다.